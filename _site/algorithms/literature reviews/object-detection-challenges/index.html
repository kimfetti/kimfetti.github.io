<!doctype html>
<html class="no-js" lang="en">
<head>
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<title>5 Significant Object Detection Challenges and Solutions</title>

    <link rel="stylesheet" type="text/css" href="http://localhost:4000//assets/css/styles_feeling_responsive.css">

  

	<script type="text/javascript" async
			src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
	</script>

    <script src="http://localhost:4000//assets/js/modernizr.min.js"></script>

	<script src="https://ajax.googleapis.com/ajax/libs/webfont/1.5.18/webfont.js"></script>
	<script>
		WebFont.load({
			google: {
				families: [ 'Lato:400,700,400italic:latin', 'Volkhov::latin' ]
			}
		});
	</script>

	<noscript>
		<link href='http://fonts.googleapis.com/css?family=Lato:400,700,400italic%7CVolkhov' rel='stylesheet' type='text/css'>
	</noscript>


	<!-- Search Engine Optimization -->
	<meta name="description" content="Object detection problems pose several unique obstacles beyond what is required for image classification.  Five such challenges are reviewed in this post along with researchers&#39; efforts to overcome these complications.">
	<meta name="google-site-verification" content="Vk0IOJ2jwG_qEoG7fuEXYqv0m2rLa8P778Fi_GrsgEQ">
	<meta name="msvalidate.01" content="0FB4C028ABCF07C908C54386ABD2D97F" >
	
	<link rel="author" href="https://plus.google.com/105464329327164540117">
	
	
	<link rel="canonical" href="http://localhost:4000//algorithms/literature%20reviews/object-detection-challenges/">


	<!-- Facebook Open Graph -->
	<meta property="og:title" content="5 Significant Object Detection Challenges and Solutions">
	<meta property="og:description" content="Object detection problems pose several unique obstacles beyond what is required for image classification.  Five such challenges are reviewed in this post along with researchers&#39; efforts to overcome these complications.">
	<meta property="og:url" content="http://localhost:4000//algorithms/literature%20reviews/object-detection-challenges/">
	<meta property="og:locale" content="en_EN">
	<meta property="og:type" content="website">
	<meta property="og:site_name" content="Kimberly Fessel Blog">
	
	<meta property="article:author" content="https://www.facebook.com/">


	
	<!-- Twitter -->
	<meta name="twitter:card" content="summary">
	<meta name="twitter:site" content="">
	<meta name="twitter:creator" content="">
	<meta name="twitter:title" content="5 Significant Object Detection Challenges and Solutions">
	<meta name="twitter:description" content="Object detection problems pose several unique obstacles beyond what is required for image classification.  Five such challenges are reviewed in this post along with researchers&#39; efforts to overcome these complications.">
	
	

	<link type="text/plain" rel="author" href="http://localhost:4000//humans.txt">

	

	

	<link rel="icon" sizes="32x32" href="http://localhost:4000//assets/img/favicon-32x32.png">

	<link rel="icon" sizes="192x192" href="http://localhost:4000//assets/img/touch-icon-192x192.png">

	<link rel="apple-touch-icon-precomposed" sizes="180x180" href="http://localhost:4000//assets/img/apple-touch-icon-180x180-precomposed.png">

	<link rel="apple-touch-icon-precomposed" sizes="152x152" href="http://localhost:4000//assets/img/apple-touch-icon-152x152-precomposed.png">

	<link rel="apple-touch-icon-precomposed" sizes="144x144" href="http://localhost:4000//assets/img/apple-touch-icon-144x144-precomposed.png">

	<link rel="apple-touch-icon-precomposed" sizes="120x120" href="http://localhost:4000//assets/img/apple-touch-icon-120x120-precomposed.png">

	<link rel="apple-touch-icon-precomposed" sizes="114x114" href="http://localhost:4000//assets/img/apple-touch-icon-114x114-precomposed.png">

	
	<link rel="apple-touch-icon-precomposed" sizes="76x76" href="http://localhost:4000//assets/img/apple-touch-icon-76x76-precomposed.png">

	<link rel="apple-touch-icon-precomposed" sizes="72x72" href="http://localhost:4000//assets/img/apple-touch-icon-72x72-precomposed.png">

	<link rel="apple-touch-icon-precomposed" href="http://localhost:4000//assets/img/apple-touch-icon-precomposed.png">	

	<meta name="msapplication-TileImage" content="http://localhost:4000//assets/img/msapplication_tileimage.png">

	<meta name="msapplication-TileColor" content="#fabb00">


	

</head>
<body id="top-of-page" class="page-fullwidth">
	
	
<div id="navigation" class="sticky">
  <nav class="top-bar" role="navigation" data-topbar>
    <ul class="title-area">
      <li class="name">
      <h1 class="show-for-small-only"><a href="http://localhost:4000/" class="icon-tree"> Kimberly Fessel Blog</a></h1>
    </li>
       <!-- Remove the class "menu-icon" to get rid of menu icon. Take out "Menu" to just have icon alone -->
      <li class="toggle-topbar menu-icon"><a href="#"><span>Nav</span></a></li>
    </ul>
    <section class="top-bar-section">

      <ul class="right">
        

              

          
          
        

              

          
          
        

              

          
          
            
            
              <li class="divider"></li>
              <li><a  href="http://localhost:4000//info/">About</a></li>

            
            
          
        

              

          
          
            
            
              <li class="divider"></li>
              <li><a  href="http://localhost:4000//contact/">Contact</a></li>

            
            
          
        
        
      </ul>

      <ul class="left">
        

              

          
          

            
            
              <li><a  href="http://localhost:4000//">Home</a></li>
              <li class="divider"></li>

            
            
          
        

              

          
          

            
            
              <li><a  href="http://localhost:4000//blog/">Blog</a></li>
              <li class="divider"></li>

            
            
          
        

              

          
          
        

              

          
          
        
        
      </ul>
    </section>
  </nav>
</div><!-- /#navigation -->

	

	

<div id="masthead-with-background-color" style="background: #999999;">
	<div class="row">
		<figure class="small-12 columns">
			<img src="https://kimfetti.github.io/images//kim_tree_header.png" alt="Kimberly Fessel Blog">
		</figure><!-- /.small-12.columns -->
	</div><!-- /.row -->
</div><!-- /#masthead -->




<div class="masthead-caption">
	Personal photo processed with YOLOv2.  Author at Haleakalā National Park.
</div>







	


<div class="row t30">
	<div class="medium-12 columns">
		<article>
			<header>
				<p class="subheadline">Reviews</p>
				<h1>5 Significant Object Detection Challenges and Solutions</h1>
			</header>

			
			<p class="teaser">
				<em>Object detection problems pose several unique obstacles beyond what is required for image classification.  Five such challenges are reviewed in this post along with researchers' efforts to overcome these complications.</em>
			</p>
			

			<!--more-->

<p>The field of computer vision has experienced substantial progress recently owing largely to advances in deep learning, specifically convolutional neural nets (CNNs).  Image classification, where a computer classifies or assigns labels to an image based on its content, can often see great results simply by leveraging pre-trained neural nets and fine-tuning the last few throughput layers.</p>

<p>Classifying <em>and</em> finding an unknown number of individual objects within an image, however, was considered an extremely difficult problem only a few years ago.  This task, called object detection, is now feasible and has even been productized by companies like <a href="https://cloud.google.com/vision/docs/drag-and-drop">Google</a> and <a href="https://www.ibm.com/watson/services/visual-recognition/">IBM</a>. But all of this progress wasn’t easy!  Object detection presents many substantial challenges beyond what is required for image classification.  After a brief introduction to  the topic, let’s take a deep dive into several of the interesting obstacles these problems face along with several emerging solutions.</p>

<h2 id="introduction">Introduction</h2>

<p>The ultimate purpose of object detection is to locate important items, draw rectangular bounding boxes around them, and determine the class of each item discovered.  Applications of object detection arise in <a href="https://www.quora.com/What-are-some-interesting-applications-of-object-detection">many different fields</a> including detecting pedestrians for self-driving cars, monitoring agricultural crops, and even real-time ball tracking for sports.  Researchers have dedicated a substantial amount of work towards this goal over the years: from Viola and Jones’s facial detection algorithm published in 2001 up to <a href="https://arxiv.org/abs/1708.02002">RetinaNet</a>, a fast, highly accurate one-state detection framework released in 2017.  The introduction of CNNs marks a pivotal moment in object detection history, as nearly all modern systems use CNNs in some form.  That said, the remainder of this post will focus on deep learning solutions for object detection, though similar challenges confront other approaches as well.</p>

<!--


### What is object detection?

While image classification has one primary classification objective, the goal of object detection is to draw rectangular bounding boxes around objects of interest as well as identify what object each box contains. A single image can consist of many different objects, so multiple bounding boxes may be drawn for each example.  Object detection [applications are basically limitless][3], but some uses include people or [animal counting][4], face detection, self-driving cars, or even ball tracking in sports.  These applications require many different kinds of objects to be detected, often with a high degree of both accuracy and speed to meet the demands of real-time video tracking.

### History

The first successful object detection frameworks relied on more traditional machine learning techniques.  These methods required extensive feature engineering to learn representative object patterns a priori before passing to a machine learning classifier like a support vector machine.  These approaches, such as the Viola-Jones algorithm, showed impressive test times and detection rates, but they often struggled to generalize to other object types or object poses since the features had to be manual engineered.

The advent of CNNs brought great improvement to the world of object detection because it allows for more robust feature sets to be learned directly from the images and allowed for nonlinear response.  Regional-proposal methods, like R-CNN introduced in 2014 and the subsequent Fast- and Faster R-CNN, depend on convolutional feature maps of select candidate regions to determine objectness (object present or not) as well as object classification.  These approaches follow two stages: 1) generate regions of interest (ROIs) where an object may be present and 2) classify this region and refine the coordinates of the ROIs.  These deep learning approaches have been so successful that the large majority of object detection leverages deep learning for at least some portion of the process, though HOG paired with tree-based methods is still performant for pedestrian detection problems (ref). The remainder of this blog post will focus on deep learning solutions for object detection, though the same challenges listed also apply to other types of approaches. 

The final category of object detection algorithms are another type of deep learning method: the so-called "single-shot detectors" such as YOLO introduced in 2016.  Rather than following the two-stage pipeline approach of regional-based methods, these systems seek to perform object detection in one shot.  This includes determining regions of interest, determining if the region contains an object or not, classifying each object detected, and refining the bounding box coordinates.  These single-shot detectors are typically much faster than the R-CNN methods, however, they often struggle with small objects and may perform worse than, say, Faster R-CNN.



One of the first successful object detection frameworks was proposed by [Viola and Jones][5] in 2001.  This system, primarily used for face detection, yielded impressive detection rates and even boasted real-time detection at 15 frames per second.  This algorithm takes advantage of the fact that human faces share similar properities.  Viola and Jones constructed a set of specifically designed [Haar Features][6] to capture facial characteristics and then fed these engineered features to a variant of AdaBoost to recognize and localize faces in test images.  While this algorithm showed impressive test times and detection rates, it suffered to generalize to other object types and changes in facial tilt.  The histogram of oriented gradients (HOG) method 


<center>
<img src="https://kimfetti.github.io/images/objdet_history.png" alt="History of Object Detection" width = "650">
<p><em> The history of object detection comprises of roughly three eras: machine learning, regional-based CNNs, and single shot detectors.  <br>Note: Many other significant approaches not listed here for brevity only.</em></p>
</center>


- History
    - Manual feature collection
    - Major gains once CNN applied to problem
    - Image depicting methods on a timeline (+ challenges overcome by each that can be referenced later)
-->

<h2 id="challenges">Challenges</h2>

<h3 id="1-dual-priorities-object-classification-and-localization">1. Dual priorities: object classification and localization</h3>

<p>The first major complication of object detection is its added goal: not only do we want to classify image objects but also to determine the objects’ positions, generally referred to as the <em>object localization</em> task.  To address this issue, researchers most often use a multi-task loss function to penalize both misclassifications and localization errors.</p>

<p>Regional-based CNNs represent one popular class of object detection frameworks.  These methods consist of the generation of region proposals where objects are likely to be located followed by CNN processing to classify and further refine object localization.  Ross Girshick et al. developed <a href="https://arxiv.org/pdf/1504.08083.pdf">Fast R-CNN</a> to improve upon their initial results with <a href="https://arxiv.org/pdf/1311.2524.pdf">R-CNN</a>. As its name implies, Fast R-CNN saw a dramatic speed-up, but accuracy also improved because the classification and localization tasks were unified into the optimization of one multi-task loss function.  Each proposed region that may contain an object is judged against the image’s true labeled objects.  Each predicted region incurs penalties for both false classification and misalignment of the bounding box.  Thus the loss function consists of two kinds of terms:</p>

<p>\[\mathcal{L}(p, u, t^u, v) = \overbrace{\mathcal{L}_c(p,u)}^{classification} + \lambda\overbrace{\left[u\geq 1\right] \mathcal{L}_l(t^u, v)}^{localization}, \]</p>

<p>where the classification term imposes log loss to the probability of the true object class \(u\) and the localization term is a smooth \(L_1\) loss for the four positional components that define the rectangle.  Note that the localization penalty does not apply when no object is present (the background class).  Also the parameter \(\lambda\) may be adjusted to prioritize either classification or localization more strongly.</p>

<!--
#### YOLO

YOLO, a single-shot detector, takes the multi-task loss function even further.  YOLO begins by laying an \\(S \times S\\) grid out on each image and allowing each grid cell \\(B\\) possible bounding boxes of varying sizes.  For each true object present in the image, the grid cell associated with the object's center is responsible for predicting this object.  The loss function thus consists of terms for each of the \\(S^2\\) grid locations, each of the \\(B\\) possible bounding boxes, and each of the \\(C\\) classes in the dataset.  Minimization of the resulting loss function allows this method to not only perform the classification and localization tasks, but also to propose regions of interest by checking if an object is present in a predefined grid cell-bounding box pair. 

The first version of YOLO primarily made localization errors, however, later iterations just penalized localization errors more heavily and saw improvement.  YOLO also  rarely produced false positives, that is, incorrectly labeling the background as an object; Fast R-CNN made many more such background errors.  Using the full image as context to both propose regions and classify them appears to be why YOLO does much better than Fast R-CNN at this since Fast R-CNN has an entirely separate ROI selection routine.
-->

<!--
#### Metrics
Another interesting consequence of having multiple objectives is the need for special metrics to evaluate object detection methods.  Two such metrics, IoU and mAP, prevail among the object detection community and are typical when reporting results or analyzing multiple approaches.

##### IoU

IoU stands for intersection over union.  This measurement judges object localization and also informs the main object detection metric, mAP.  IoU compares the actual and predicted bounding boxes by calculating the area of their overlap divided by the total area of these two boxes.  Generally, IoU above 50% is defined as a positive match.  

<center>
<img src="https://kimfetti.github.io/images/iou.png" alt="Intersection over Union" width = "500">
<p><em> The intersection over union metric judges the exactness of the object localization task.</em></p>
</center>

##### mAP

mAP, or mean average precision, on the other hand, assesses the classification task. mAP computes the mean of the average precision (AP) across all object classes in the dataset.  Once a list of bounding box predictions have been constructed, those meeting a prescribed IOU level (often 50%) are deemed positives while others are negatives.  Now beginning with the precision-recall (PR) curve for just one object class, AP approximates the area under the PR curve:

\\[ AP = \int_0^1 p(r) dr,\\]

where \\(p \equiv\\) precision and \\(r \equiv\\) recall.  The exact details of this calculation varies a bit between datasets--COCO uses 101-point interpolation, for example--but overall, AP attempts to aggregate precision over all values of recall, from zero to one.  mAP then is just the numerical mean of each of these AP values for every object class in the dataset.  For more a more robust discussion on how mAP is defined for each  dataset, check out [this blog post][7]. 

-->

<!--
- Regions of interest (independent of class, pipeline solution)
- Multi-task loss function
- Multiple objects in one image
- New metrics: IOU
-->

<h3 id="2-speed-for-real-time-detection">2. Speed for real-time detection</h3>

<p>Object detection algorithms need to not only be accurate when classifying and localizing important objects, they also need to be incredibly fast at prediction time to meet the real-time demands of video processing.  Several key enhancements over the years have boosted the speed of these algorithms, improving test time from the 0.02 frames per second (fps) of R-CNN to the impressive 155 fps of Fast YOLO.</p>

<p>As the names imply, Fast R-CNN and Faster R-CNN were built to speed up the original R-CNN method.  R-CNN uses <a href="https://koen.me/research/pub/uijlings-ijcv2013-draft.pdf">selective search</a> to generate 2,000 candidate regions of interest (ROIs) and passes each ROI through a CNN base individually causing a massive bottleneck since this CNN processing is quite slow. Fast R-CNN instead sends the entire image through the CNN base just once and then matches the ROIs created with selective search to the CNN feature map, yielding a 20-fold reduction in processing time.  While Fast R-CNN is much speedier than R-CNN, yet another bottleneck persists.  It takes approxiamtely 2.3 seconds for Fast R-CNN to perform object detection on a single image, and selective search accounts for a full 2 seconds of that time!  Faster R-CNN replaces selective search with a separate sub-neural network to generate ROIs, creating another 10x speed up and thus testing at a rate of about 7-18 fps.</p>

<!--
The first major improvements in speed come from the R-CNN, Fast R-CNN, and Faster R-CNN systems, all developed by Ross Girshick's group.  R-CNN uses selective search to generate 2,000 proposal ROIs.  Each ROI is then processed through CNN layers to then refine the bounding box coordinates and classify each found object.  A huge bottleneck in this approach is that each of the 2000 ROIs must be processed with the CNN base individually.  Fast R-CNN solves this speed issue by first processing the entire image with the CNN base to build a feature map for the entire image.  The ROIs generated by selective search are then paired to the appropriate location on the feature map before processing with the final layers.  This reduction in passes through the CNN base yields a 20-fold reduction in processing time.

While Fast R-CNN is much speedier than R-CNN, yet another bottleneck persists: the initial creation of the region proposals with selective search.  It takes approximately 2.3 seconds for each image to be processed with Fast R-CNN, and selective search accounts for a full 2 seconds of that time!  Faster R-CNN eliminates this process and generates ROIs with a separate sub-neural network.  _Inital guesses for bounding boxes are allowed to be less precise knowing that the downstream regression task will correct these localization errors._ This change creates another 10X speed-up, and this Faster R-CNN method tests at a rate of about 7-18 fps.
-->

<p>Despite these impressive improvements to R-CNN, videos are typically shot at at least 24 fps, meaning Faster R-CNN will likely not keep pace.  Regional-based methods consist of two separate phases: proposing regions and processing them. This task separation proves to be somewhat inefficient.  Another major type of object detection systems relies on a unified one-state approach instead.  These so-called single-shot detectors aim to fully locate and classify objects during a single pass ove the image, thus substantially decreasing test time.  One such single-shot detector YOLO begins by laying out a grid over the image and allows each grid cell to detect a fixed number of objects of varying sizes.  For each true object present in the image, the grid cell associated with the object’s center is responsible for predicting this object.  A complex, multi-term loss function then ensures that all localization and classification occurs within one process.  One version of this method, Fast YOLO, has even achieved rates of 155 fps; however, classification and localization accuracy drops off sharply at this elevated speed.</p>

<!--
While this means we have cut test time from 49 seconds per image to about 0.2 seconds which is quite impressive, videos are typically shot at at least 24 fps, so as it stands, Faster R-CNN will not be able to keep pace.  The final bottleneck to overcome in Faster R-CNN is the separate components of the regional proposal network and the detection network.  Single-shot detectors, on the other hand, create region proposals in the same pass as the classification and localization tasks thus dramatically decreasing test time per image.  Fast YOLO has even been able to achieve rates of 155 fps; however, reaching such speeds certainly comes with a cost as classification and localization accuracy sharply drop off at these speeds.  
-->

<p>Ultimately, today’s object detection algorithms attempt to strike a balance between speed and accuracy.  Several design choices beyond the detection framework influence these outcomes.  For example, YOLOv3 allows images of varying resolutions–high-res images typically see better accuracy but slower processing times.  The choice of the CNN base also affects the speed-accuracy tradeoff.  Very deep networks like the 164 layers used in Inception-ResNet-V2 yield impressive accuracy, but pale in comparision to frameworks with VGG-16 in terms of speed.  Object detection design choices are made in context depending on whether speed or accuracy takes priority.</p>

<!--
_Faster R-CNN still better accuracy than SSDs in general_
- Heading toward RT detection in videos -- need to process images very quickly
- Fast R-CNN (process image through CNN first)
- Faster R-CNN (separate RPN)
- YOLO (multi-task optimization, all in one go, no alternate optimization like Faster R-CNN)
- SSD?
- Truncated SVD
- Trade speed and accuracy
-->

<h3 id="3-multiple-spatial-scales-and-aspect-ratios">3. Multiple spatial scales and aspect ratios</h3>
<!--
- Warping of ROI before being fed into CNN (R-CNN)
- SPP layer
- Anchors
-->

<p>Another big challenge in object detection is the fact that objects of interest may come in a wide range of sizes and aspect ratios.  Several techniques have been tried to address these issues.</p>

<h4 id="anchor-boxes">Anchor boxes</h4>

<p>With its updated region proposal network, Faster R-CNN employs anchor boxes as initial guesses for its RoIs.  Anchor boxes are distributed throughout the image and <em>used to initialize the RPN</em>.  The shape and sizes of these boxes are carefully chosen to span a range of different sizes and aspect ratios with the hopes that all types of objects can be detected and the coordinates need not be updated too much during the optimization.  Other frameworks, including single-shot detectors, have leveraged these anchor boxes as starting points for RoI selection.</p>

<center>
<img src="https://kimfetti.github.io/images/anchors.png" alt="Anchor boxes as initial RoIs" width="500" />
<p><em> Carefully chosen anchor boxes of varying size and aspect ratio can be used to make initial guesses for the regions of interest and help detect objects of different sizes and shapes.</em></p>
</center>

<h4 id="multiple-feature-maps">Multiple feature maps</h4>

<p>Single-shot detectors need to pay special consideration to this issue of multiple scales because they need to not only classify objects and adjust bounding boxes but also come up with the regions of interest all in one shot from the CNN.  If only the final CNN layers are used to look for objects, only the largest objects will be found because smaller objects can be lost during the downsampling of the pooling layers.  To solve this problem and be able to detect smaller objects, single-shot detectors typically detect objects using multiple different CNN layers including earlier layers that have not yet lost as much resolution.  Predictions can either be made independently and fused together (SSD) or multiple layers can be concatenated before predictions are made (YOLO).  Even with these precautions, single-shot detectors are notoriously bad at detecting small objects, especially those tight groupings like a flock of birds.  The third and most recent version of YOLO appears to have corrected this shortcoming a bit, but all detection methods tend to perform better for larger objects in general.  Increasing input image resolution may also help with small object accuracy.</p>

<center>
<img src="https://kimfetti.github.io/images/ssd.png" alt="SSD with multiple feature maps" width="800" />
<p><em> Feature maps from multiple layers of the SSD CNN are used to make object detections at multiple scales.</em></p>
</center>

<h4 id="feature-pyramid-network">Feature pyramid network</h4>

<p>The <a href="https://arxiv.org/pdf/1612.03144.pdf">feature pyramid network (FPN)</a> concept takes this idea of multiple feature layers one step further.  Images first pass through the typical CNN pathway so that the final layers are more semantically rich.  Then to obtain better resolution and localization of objects learned, a top down pathway is also implemented thus upsampling the feature map and regaining higher resolution.  While this top down pathway is good for learning objects of varying sizes, spatial positions can get skewed.  To improve the localization of objects detected, lateral connections are added between the original feature maps and the corresponding reconstructed layers.  FPN provides one of the strongest ways to detect objects of varying sizes and this technique was added to YOLO in version 3.</p>

<center>
<img src="https://kimfetti.github.io/images/fpn.png" alt="Feature pyramid network" width="450" />
<p><em> The feature pyramid network is able to detect objects of varying sizes by reconstructing higher resolution layers from those with semantic strength.</em></p>
</center>

<!--
### 3. Spatial position IS relevant 
- Image classification does not do this
- Fully connected layers (slow)
- ?? (I forgot this solution... )
-->

<h3 id="4-limited-data">4. Limited data</h3>

<p>One of the biggest hurdles for object detection thus far is the unfortunate limited amount of annotated data available.  Dataset for object detection typicallky contain ground truth examples of a dozen to a hundred classes of objects, as compared to image classification datasets with upwards of 100,000 different classes.  Tags for image classification are often solicited and provided by users for free (think of parsing the text of someone’s vacation photo captions); whereas, gathering ground truth labels and bounding boxes for object detection is incredibly tedious work.</p>

<p>One of the leading currently available datasets for object detection is the COCO dataset provided by Microsoft.  This set contains 300,000 segmented images with <a href="https://github.com/pjreddie/darknet/blob/master/data/coco.names">80 different categories</a> of objects at broad scales with very precise labeled locations.  Images contain about 7 objects each on average.  While this dataset is incredibly helpful, if someone is looking for objects not contained within the 80 selected items, they won’t be able to find them by training solely on this dataset.</p>

<p>A very interesting approach at solving for this problem comes from YOLO9000, or the second version of YOLO.  This work contains several important updates for YOLO but it also aims to narrow difference in dataset sizes between image classification and detection as it is trained simultaneously both on COCO and <a href="http://www.image-net.org/">ImageNet</a>, a tagged dataset tens of thousands of object classes, typically reserved for image classification.  YOLO9000 uses the COCO detection information to precisely locate objects and the classification images to then increase the algorithm’s “vocabulary.” YOLO9000 leverages the fact that ImageNet’s labels are taken from WordNet and contain a very hierarchical structure.  A hierarchical WordTree is this built to first detect an object’s concept (such as “dog”) and then uses a softmax function on the predicted class probabilities to drill down into specifics (such as “Siberian husky”).  Overall, this approach appears to work well for concepts well known to COCO, such as animals, but performs more poorly for new types of concepts such as equipment or clothing since these types of items are not detected during the objectness phase and are not found in COCO.</p>

<center>
<img src="https://kimfetti.github.io/images/yolo9000.png" alt="YOLO9000 WordTree and examples" width="700" />
<p><em> YOLO9000 training is trained with both COCO for accurate detection and ImageNet for increase classification options.</em></p>
</center>

<!--
- Lots of data for image classification (ImageNet), not so much for image detection (COCO)
- YOLO9000 attempt to leverage both for training
-->

<h3 id="5-class-imbalance">5. Class imbalance</h3>

<p>Class imbalance is an issue for most classification problems, and object detection also feels this pain.  Consider a typical photograph.  More likely than not, this typical photograph will contain a few main objects and the remainder of the image will be part of the background.  R-CNN begins with 2000 candidate ROIs per image–just imagine how many of these regions don’t contain an object and are considered negatives!</p>

<p>Rather than continuing to learn more about the background regions, hard example mining filters the negative examples done to those that the model performs worst one.  Some approaches also cap the ratio of picked negatives (background) to positives (objects), say, no greater than 3:1.</p>

<p>Non-maximal suppression (NMS) is also used by many object detection algorithms to correct for the fact that one object may be detected multiple times by the proposed RoIs.  With NMS the ….</p>

<p>More recently focal loss has been used to reduce the effects of class imbalance.  Focal loss replaces the traditional classification log loss as
\[ FL(p_u) = -(1-p_u)^\gamma \log(p_u)\]
where \(p_u \equiv \) predicted class probability for the actual true class and \(\gamma &gt; 0\).  The effect of this additional factor reduces the loss for well-classified examples, thus deemphasizing observations with high class confidence, such as regions that clearly contain background.  This helps deemphasize classes with many examples that the model knows well, such as the background.</p>

<h2 id="conclusion">Conclusion</h2>
<p>Object detection turns out to be a much harder than image classification tasks, particularly because of these five challenges: dual priorities, speed, multiple scales, limited data, and class imbalance.  Researchers have dedicated much effort to address these challenges and have presented some amazing solutions thus far.  A primary design decision still persists: the prioritization of accuracy for boosted mAP or speed.  Two of the most powerful object detection frameworks currently in play included RetinaNet with a complex CNN base, FPN for detecting multiple scales, and focal loss to adjust for class imbalance.  This method shows impressive accuracy scores at <em>how accurate is it?</em>  YOLO also continues to be popular among the object detection community due to its speed in processing up to 100+ fps.  Several great improvements have been introduced to YOLO in the last few years making the third version just as accurate as SSD but three times faster.  It also shows similar performance compared to RetinaNet if comparing AP\(_{50}\).</p>

<p>Some challenges still persist, however.  Small objects are still difficult for any of these frameworks.  It is especially difficult to detect small objects bunched together in a group due to resulting partial occlusions.  Real-time detection at a top-level accuracy (both classification and localization) is still a challenge as well.  Current methods often need to sacrifice one or the other, so researchers are still trying to marry the two.  Another interesting challenge which may see more research is extending object detection from 2D bounding boxes to 3D bounding cubes.  Furthermore, video tracking may also see important improvements in the future.  Rather than processing each frame of a video, perhaps we can take advantage of some assumed continuity between frames to reduce computation and smooth out object detections from frame to frame.  Event though many interesting challenges have seen creative solutions, all of these additional challenges and many more mean that object detection research is certainly not done!</p>

<!--
- Much harder than image classification tasks
- Future challenges like adding LSTM, time component to video processing.  Currently one frame at a time
- Marry speed and accuracy AND extend object class space
- Also: pose, occlusions, lighting (same issues that image classification has but maybe even more so since also trying to localize object)
- Check out review's future stuff again
-->



			
						<div id="page-meta" class="t30">
				<p>
					<!-- Look the author details up from the site config. -->
					
					<!-- Output author details if some exist. -->
					

				
				<time class="icon-calendar pr20" datetime="2019-08-13T00:00:00-04:00" itemprop="datePublished"> 2019-08-13</time>
				

				<span class="icon-archive pr20"> ALGORITHMS · LITERATURE REVIEWS</span>
				<br />
				<span class="pr20"></span>
			</p>

			
			<div id="post-nav" class="row">
				
				<div class="small-5 columns"><a class="button small radius prev" href="http://localhost:4000//visualizations/matplotlib-improvements/">&laquo; Simple Ways to Improve Your Matplotlib</a></div><!-- /.small-4.columns -->
				
				<div class="small-2 columns text-center"><a class="radius button small" href="http://localhost:4000//blog/archive/" title="Blog Archive">Archive</a></div><!-- /.small-4.columns -->
				
				<div class="small-5 columns text-right"></div><!-- /.small-4.columns -->
				
			</div>
			
			</div><!--  /.page-meta -->

			

			
						
				<h3 id="comments" class="t60">Dialogue &amp; Discussion</h3>
			    <div id="disqus_thread"></div>
			    <script type="text/javascript">
			        /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
			        var disqus_shortname = 'kimfetti-github-io'; 
			        var disqus_identifier = '/algorithms/literature%20reviews/object-detection-challenges/';

			        /* * * DON'T EDIT BELOW THIS LINE * * */
			        (function() {
			            var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
			            dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
			            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
			        })();
			    </script>
			    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
			



			

		</article>
	</div><!-- /.medium-12.columns -->
</div><!-- /.row -->




	
	    <div id="up-to-top" class="row">
      <div class="small-12 columns" style="text-align: right;">
        <a class="iconfont" href="#top-of-page">&#xf108;</a>
      </div><!-- /.small-12.columns -->
    </div><!-- /.row -->


    <footer id="footer-content" class="bg-grau">
      <div id="footer">
        <div class="row">
          <div class="medium-6 large-5 columns">
            <h5 class="shadow-black">About This Site</h5>

            <p class="shadow-black">
              Kimberly Fessel is a Senior Data Scientist at Metis. Her enthusiasm for data storytelling often leads her toward better math, better visuals, and better science!
              <a href="http://localhost:4000//info/">More ›</a>
            </p>
          </div><!-- /.large-6.columns -->


          <div class="small-6 medium-3 large-3 large-offset-1 columns">
            
              
                <h5 class="shadow-black">Extras</h5>
              
            
              
            
              
            
              
            

              <ul class="no-bullet shadow-black">
              
                
                  <li >
                    <a href="http://localhost:4000/"  title=""></a>
                  </li>
              
                
                  <li >
                    <a href="http://localhost:4000//info/"  title="Learn more about Kimberly">About</a>
                  </li>
              
                
                  <li >
                    <a href="http://localhost:4000//contact/"  title="Contact Kimberly">Contact</a>
                  </li>
              
                
                  <li >
                    <a href="https://www.thisismetis.com" target="_blank"  title="Learn more about Metis">This is Metis</a>
                  </li>
              
              </ul>
          </div><!-- /.large-4.columns -->


          <div class="small-6 medium-3 large-3 columns">
            
              
                <h5 class="shadow-black">Thank You</h5>
              
            
              
            

            <ul class="no-bullet shadow-black">
            
              
                <li >
                  <a href="http://localhost:4000/"  title=""></a>
                </li>
            
              
                <li class="network-entypo" >
                  <a href="http://www.connellypartners.com/who-we-are/" target="_blank"  title="Photoshoot @ CP">Photoshoot @ CP</a>
                </li>
            
            </ul>
          </div><!-- /.large-3.columns -->
        </div><!-- /.row -->

      </div><!-- /#footer -->


      <div id="subfooter">
        <nav class="row">
          <section id="subfooter-left" class="small-12 medium-6 columns credits">
            <p>Template with &hearts; by <a href="http://phlow.de/">Phlow</a> with <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> based on <a href="http://phlow.github.io/feeling-responsive/">Feeling Responsive</a>.</p>
          </section>

          <section id="subfooter-right" class="small-12 medium-6 columns">
            <ul class="inline-list social-icons">
            
              <li><a href="https://www.linkedin.com/in/kimberlyfessel/" target="_blank" class="icon-linkedin" title="Professional profile"></a></li>
            
              <li><a href="http://github.com/kimfetti" target="_blank" class="icon-github" title="Code and more"></a></li>
            
            </ul>
          </section>
        </nav>
      </div><!-- /#subfooter -->
    </footer>

	

	


<script src="http://localhost:4000//assets/js/javascript.min.js"></script>







<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-131656438-1', 'auto');
  ga('set', 'anonymizeIp', true);
  ga('send', 'pageview');

</script>








</body>
</html>

