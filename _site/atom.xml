<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="//assets/xslt/atom.xslt" ?>
<?xml-stylesheet type="text/css" href="//assets/css/atom.css" ?>
<feed xmlns="http://www.w3.org/2005/Atom">
	<id>http://localhost:4000//</id>
	<title>Kimberly Fessel Blog</title>
	<updated>2019-09-09T20:25:08-04:00</updated>

	<subtitle>Kimberly Fessel is a Senior Data Scientist at Metis. Her enthusiasm for data storytelling often leads her toward better math, better visuals, and better science!</subtitle>

	
		
		<author>
			
				<name>Kimberly Fessel</name>
			
			
			
		</author>
	

	<link href="http://localhost:4000//atom.xml" rel="self" type="application/rss+xml" />
	<link href="http://localhost:4000//" rel="alternate" type="text/html" />

	<generator uri="http://jekyllrb.com" version="3.3.1">Jekyll</generator>

	
		<entry>
			<id>http://localhost:4000//algorithms/literature%20reviews/object-detection-challenges/</id>
			<title>5 Significant Object Detection Challenges and Solutions</title>
			<link href="http://localhost:4000//algorithms/literature%20reviews/object-detection-challenges/" rel="alternate" type="text/html" title="5 Significant Object Detection Challenges and Solutions" />
			<updated>2019-08-13T00:00:00-04:00</updated>

			
				
				<author>
					
						<name>KFessel</name>
					
					
					
				</author>
			
			<summary>&lt;em&gt;Object detection problems pose several unique obstacles beyond what is required for image classification.  Five such challenges are reviewed in this post along with researchers' efforts to overcome these complications.&lt;/em&gt;</summary>
			<content type="html" xml:base="http://localhost:4000//algorithms/literature%20reviews/object-detection-challenges/">&lt;!--more--&gt;

&lt;p&gt;The field of computer vision has experienced substantial progress recently owing largely to advances in deep learning, specifically convolutional neural nets (CNNs).  Image classification, where a computer classifies or assigns labels to an image based on its content, can often see great results simply by leveraging pre-trained neural nets and fine-tuning the last few throughput layers.&lt;/p&gt;

&lt;p&gt;Classifying &lt;em&gt;and&lt;/em&gt; finding an unknown number of individual objects within an image, however, was considered an extremely difficult problem only a few years ago.  This task, called object detection, is now feasible and has even been productized by companies like &lt;a href=&quot;https://cloud.google.com/vision/docs/drag-and-drop&quot;&gt;Google&lt;/a&gt; and &lt;a href=&quot;https://www.ibm.com/watson/services/visual-recognition/&quot;&gt;IBM&lt;/a&gt;. But all of this progress wasn’t easy!  Object detection presents many substantial challenges beyond what is required for image classification.  After a brief introduction to  the topic, let’s take a deep dive into several of the interesting obstacles these problems face along with several emerging solutions.&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;The ultimate purpose of object detection is to locate important items, draw rectangular bounding boxes around them, and determine the class of each item discovered.  Applications of object detection arise in &lt;a href=&quot;https://www.quora.com/What-are-some-interesting-applications-of-object-detection&quot;&gt;many different fields&lt;/a&gt; including detecting pedestrians for self-driving cars, monitoring agricultural crops, and even real-time ball tracking for sports.  Researchers have dedicated a substantial amount of work towards this goal over the years: from Viola and Jones’s facial detection algorithm published in 2001 up to &lt;a href=&quot;https://arxiv.org/abs/1708.02002&quot;&gt;RetinaNet&lt;/a&gt;, a fast, highly accurate one-state detection framework released in 2017.  The introduction of CNNs marks a pivotal moment in object detection history, as nearly all modern systems use CNNs in some form.  That said, the remainder of this post will focus on deep learning solutions for object detection, though similar challenges confront other approaches as well.&lt;/p&gt;

&lt;!--


### What is object detection?

While image classification has one primary classification objective, the goal of object detection is to draw rectangular bounding boxes around objects of interest as well as identify what object each box contains. A single image can consist of many different objects, so multiple bounding boxes may be drawn for each example.  Object detection [applications are basically limitless][3], but some uses include people or [animal counting][4], face detection, self-driving cars, or even ball tracking in sports.  These applications require many different kinds of objects to be detected, often with a high degree of both accuracy and speed to meet the demands of real-time video tracking.

### History

The first successful object detection frameworks relied on more traditional machine learning techniques.  These methods required extensive feature engineering to learn representative object patterns a priori before passing to a machine learning classifier like a support vector machine.  These approaches, such as the Viola-Jones algorithm, showed impressive test times and detection rates, but they often struggled to generalize to other object types or object poses since the features had to be manual engineered.

The advent of CNNs brought great improvement to the world of object detection because it allows for more robust feature sets to be learned directly from the images and allowed for nonlinear response.  Regional-proposal methods, like R-CNN introduced in 2014 and the subsequent Fast- and Faster R-CNN, depend on convolutional feature maps of select candidate regions to determine objectness (object present or not) as well as object classification.  These approaches follow two stages: 1) generate regions of interest (ROIs) where an object may be present and 2) classify this region and refine the coordinates of the ROIs.  These deep learning approaches have been so successful that the large majority of object detection leverages deep learning for at least some portion of the process, though HOG paired with tree-based methods is still performant for pedestrian detection problems (ref). The remainder of this blog post will focus on deep learning solutions for object detection, though the same challenges listed also apply to other types of approaches. 

The final category of object detection algorithms are another type of deep learning method: the so-called &quot;single-shot detectors&quot; such as YOLO introduced in 2016.  Rather than following the two-stage pipeline approach of regional-based methods, these systems seek to perform object detection in one shot.  This includes determining regions of interest, determining if the region contains an object or not, classifying each object detected, and refining the bounding box coordinates.  These single-shot detectors are typically much faster than the R-CNN methods, however, they often struggle with small objects and may perform worse than, say, Faster R-CNN.



One of the first successful object detection frameworks was proposed by [Viola and Jones][5] in 2001.  This system, primarily used for face detection, yielded impressive detection rates and even boasted real-time detection at 15 frames per second.  This algorithm takes advantage of the fact that human faces share similar properities.  Viola and Jones constructed a set of specifically designed [Haar Features][6] to capture facial characteristics and then fed these engineered features to a variant of AdaBoost to recognize and localize faces in test images.  While this algorithm showed impressive test times and detection rates, it suffered to generalize to other object types and changes in facial tilt.  The histogram of oriented gradients (HOG) method 


&lt;center&gt;
&lt;img src=&quot;https://kimfetti.github.io/images/objdet_history.png&quot; alt=&quot;History of Object Detection&quot; width = &quot;650&quot;&gt;
&lt;p&gt;&lt;em&gt; The history of object detection comprises of roughly three eras: machine learning, regional-based CNNs, and single shot detectors.  &lt;br&gt;Note: Many other significant approaches not listed here for brevity only.&lt;/em&gt;&lt;/p&gt;
&lt;/center&gt;


- History
    - Manual feature collection
    - Major gains once CNN applied to problem
    - Image depicting methods on a timeline (+ challenges overcome by each that can be referenced later)
--&gt;

&lt;h2 id=&quot;challenges&quot;&gt;Challenges&lt;/h2&gt;

&lt;h3 id=&quot;1-dual-priorities-object-classification-and-localization&quot;&gt;1. Dual priorities: object classification and localization&lt;/h3&gt;

&lt;p&gt;The first major complication of object detection is its added goal: not only do we want to classify image objects but also to determine the objects’ positions, generally referred to as the &lt;em&gt;object localization&lt;/em&gt; task.  To address this issue, researchers most often use a multi-task loss function to penalize both misclassifications and localization errors.&lt;/p&gt;

&lt;p&gt;Regional-based CNNs represent one popular class of object detection frameworks.  These methods consist of the generation of region proposals where objects are likely to be located followed by CNN processing to classify and further refine object localization.  Ross Girshick et al. developed &lt;a href=&quot;https://arxiv.org/pdf/1504.08083.pdf&quot;&gt;Fast R-CNN&lt;/a&gt; to improve upon their initial results with &lt;a href=&quot;https://arxiv.org/pdf/1311.2524.pdf&quot;&gt;R-CNN&lt;/a&gt;. As its name implies, Fast R-CNN saw a dramatic speed-up, but accuracy also improved because the classification and localization tasks were unified into the optimization of one multi-task loss function.  Each proposed region that may contain an object is judged against the image’s true labeled objects.  Each predicted region incurs penalties for both false classification and misalignment of the bounding box.  Thus the loss function consists of two kinds of terms:&lt;/p&gt;

&lt;p&gt;\[\mathcal{L}(p, u, t^u, v) = \overbrace{\mathcal{L}_c(p,u)}^{classification} + \lambda\overbrace{\left[u\geq 1\right] \mathcal{L}_l(t^u, v)}^{localization}, \]&lt;/p&gt;

&lt;p&gt;where the classification term imposes log loss to the probability of the true object class \(u\) and the localization term is a smooth \(L_1\) loss for the four positional components that define the rectangle.  Note that the localization penalty does not apply when no object is present (the background class).  Also the parameter \(\lambda\) may be adjusted to prioritize either classification or localization more strongly.&lt;/p&gt;

&lt;!--
#### YOLO

YOLO, a single-shot detector, takes the multi-task loss function even further.  YOLO begins by laying an \\(S \times S\\) grid out on each image and allowing each grid cell \\(B\\) possible bounding boxes of varying sizes.  For each true object present in the image, the grid cell associated with the object's center is responsible for predicting this object.  The loss function thus consists of terms for each of the \\(S^2\\) grid locations, each of the \\(B\\) possible bounding boxes, and each of the \\(C\\) classes in the dataset.  Minimization of the resulting loss function allows this method to not only perform the classification and localization tasks, but also to propose regions of interest by checking if an object is present in a predefined grid cell-bounding box pair. 

The first version of YOLO primarily made localization errors, however, later iterations just penalized localization errors more heavily and saw improvement.  YOLO also  rarely produced false positives, that is, incorrectly labeling the background as an object; Fast R-CNN made many more such background errors.  Using the full image as context to both propose regions and classify them appears to be why YOLO does much better than Fast R-CNN at this since Fast R-CNN has an entirely separate ROI selection routine.
--&gt;

&lt;!--
#### Metrics
Another interesting consequence of having multiple objectives is the need for special metrics to evaluate object detection methods.  Two such metrics, IoU and mAP, prevail among the object detection community and are typical when reporting results or analyzing multiple approaches.

##### IoU

IoU stands for intersection over union.  This measurement judges object localization and also informs the main object detection metric, mAP.  IoU compares the actual and predicted bounding boxes by calculating the area of their overlap divided by the total area of these two boxes.  Generally, IoU above 50% is defined as a positive match.  

&lt;center&gt;
&lt;img src=&quot;https://kimfetti.github.io/images/iou.png&quot; alt=&quot;Intersection over Union&quot; width = &quot;500&quot;&gt;
&lt;p&gt;&lt;em&gt; The intersection over union metric judges the exactness of the object localization task.&lt;/em&gt;&lt;/p&gt;
&lt;/center&gt;

##### mAP

mAP, or mean average precision, on the other hand, assesses the classification task. mAP computes the mean of the average precision (AP) across all object classes in the dataset.  Once a list of bounding box predictions have been constructed, those meeting a prescribed IOU level (often 50%) are deemed positives while others are negatives.  Now beginning with the precision-recall (PR) curve for just one object class, AP approximates the area under the PR curve:

\\[ AP = \int_0^1 p(r) dr,\\]

where \\(p \equiv\\) precision and \\(r \equiv\\) recall.  The exact details of this calculation varies a bit between datasets--COCO uses 101-point interpolation, for example--but overall, AP attempts to aggregate precision over all values of recall, from zero to one.  mAP then is just the numerical mean of each of these AP values for every object class in the dataset.  For more a more robust discussion on how mAP is defined for each  dataset, check out [this blog post][7]. 

--&gt;

&lt;!--
- Regions of interest (independent of class, pipeline solution)
- Multi-task loss function
- Multiple objects in one image
- New metrics: IOU
--&gt;

&lt;h3 id=&quot;2-speed-for-real-time-detection&quot;&gt;2. Speed for real-time detection&lt;/h3&gt;

&lt;p&gt;Object detection algorithms need to not only be accurate when classifying and localizing important objects, they also need to be incredibly fast at prediction time to meet the real-time demands of video processing.  Several key enhancements over the years have boosted the speed of these algorithms, improving test time from the 0.02 frames per second (fps) of R-CNN to the impressive 155 fps of Fast YOLO.&lt;/p&gt;

&lt;p&gt;As the names imply, Fast R-CNN and Faster R-CNN were built to speed up the original R-CNN method.  R-CNN uses &lt;a href=&quot;https://koen.me/research/pub/uijlings-ijcv2013-draft.pdf&quot;&gt;selective search&lt;/a&gt; to generate 2,000 candidate regions of interest (ROIs) and passes each ROI through a CNN base individually causing a massive bottleneck since this CNN processing is quite slow. Fast R-CNN instead sends the entire image through the CNN base just once and then matches the ROIs created with selective search to the CNN feature map, yielding a 20-fold reduction in processing time.  While Fast R-CNN is much speedier than R-CNN, yet another bottleneck persists.  It takes approxiamtely 2.3 seconds for Fast R-CNN to perform object detection on a single image, and selective search accounts for a full 2 seconds of that time!  Faster R-CNN replaces selective search with a separate sub-neural network to generate ROIs, creating another 10x speed up and thus testing at a rate of about 7-18 fps.&lt;/p&gt;

&lt;!--
The first major improvements in speed come from the R-CNN, Fast R-CNN, and Faster R-CNN systems, all developed by Ross Girshick's group.  R-CNN uses selective search to generate 2,000 proposal ROIs.  Each ROI is then processed through CNN layers to then refine the bounding box coordinates and classify each found object.  A huge bottleneck in this approach is that each of the 2000 ROIs must be processed with the CNN base individually.  Fast R-CNN solves this speed issue by first processing the entire image with the CNN base to build a feature map for the entire image.  The ROIs generated by selective search are then paired to the appropriate location on the feature map before processing with the final layers.  This reduction in passes through the CNN base yields a 20-fold reduction in processing time.

While Fast R-CNN is much speedier than R-CNN, yet another bottleneck persists: the initial creation of the region proposals with selective search.  It takes approximately 2.3 seconds for each image to be processed with Fast R-CNN, and selective search accounts for a full 2 seconds of that time!  Faster R-CNN eliminates this process and generates ROIs with a separate sub-neural network.  _Inital guesses for bounding boxes are allowed to be less precise knowing that the downstream regression task will correct these localization errors._ This change creates another 10X speed-up, and this Faster R-CNN method tests at a rate of about 7-18 fps.
--&gt;

&lt;p&gt;Despite these impressive improvements to R-CNN, videos are typically shot at at least 24 fps, meaning Faster R-CNN will likely not keep pace.  Regional-based methods consist of two separate phases: proposing regions and processing them. This task separation proves to be somewhat inefficient.  Another major type of object detection systems relies on a unified one-state approach instead.  These so-called single-shot detectors aim to fully locate and classify objects during a single pass ove the image, thus substantially decreasing test time.  One such single-shot detector YOLO begins by laying out a grid over the image and allows each grid cell to detect a fixed number of objects of varying sizes.  For each true object present in the image, the grid cell associated with the object’s center is responsible for predicting this object.  A complex, multi-term loss function then ensures that all localization and classification occurs within one process.  One version of this method, Fast YOLO, has even achieved rates of 155 fps; however, classification and localization accuracy drops off sharply at this elevated speed.&lt;/p&gt;

&lt;!--
While this means we have cut test time from 49 seconds per image to about 0.2 seconds which is quite impressive, videos are typically shot at at least 24 fps, so as it stands, Faster R-CNN will not be able to keep pace.  The final bottleneck to overcome in Faster R-CNN is the separate components of the regional proposal network and the detection network.  Single-shot detectors, on the other hand, create region proposals in the same pass as the classification and localization tasks thus dramatically decreasing test time per image.  Fast YOLO has even been able to achieve rates of 155 fps; however, reaching such speeds certainly comes with a cost as classification and localization accuracy sharply drop off at these speeds.  
--&gt;

&lt;p&gt;Ultimately, today’s object detection algorithms attempt to strike a balance between speed and accuracy.  Several design choices beyond the detection framework influence these outcomes.  For example, YOLOv3 allows images of varying resolutions–high-res images typically see better accuracy but slower processing times.  The choice of the CNN base also affects the speed-accuracy tradeoff.  Very deep networks like the 164 layers used in Inception-ResNet-V2 yield impressive accuracy, but pale in comparision to frameworks with VGG-16 in terms of speed.  Object detection design choices are made in context depending on whether speed or accuracy takes priority.&lt;/p&gt;

&lt;!--
_Faster R-CNN still better accuracy than SSDs in general_
- Heading toward RT detection in videos -- need to process images very quickly
- Fast R-CNN (process image through CNN first)
- Faster R-CNN (separate RPN)
- YOLO (multi-task optimization, all in one go, no alternate optimization like Faster R-CNN)
- SSD?
- Truncated SVD
- Trade speed and accuracy
--&gt;

&lt;h3 id=&quot;3-multiple-spatial-scales-and-aspect-ratios&quot;&gt;3. Multiple spatial scales and aspect ratios&lt;/h3&gt;
&lt;!--
- Warping of ROI before being fed into CNN (R-CNN)
- SPP layer
- Anchors
--&gt;

&lt;p&gt;For many applications of object detection, items of interest may appear in a wide range of sizes and aspect ratios.  Practitioners leverage several techniques to ensure detection algorithms are able to capture objects at multiple scales and views.&lt;/p&gt;

&lt;h4 id=&quot;anchor-boxes&quot;&gt;Anchor boxes&lt;/h4&gt;

&lt;p&gt;Instead of selective search, Faster R-CNN’s updated region proposal network uses a small sliding window across the image’s convolutional feature map to generate candidate RoIs.  Multiple RoIs are predicted at each position and are described relative to the so-called anchor boxes.  The shape and sizes of these anchor boxes are carefully chosen to span a range of different sizes and aspect ratios.  This allows various types of objects to be detected with the hopes that the bounding box coordinates need not be updated much during the localization task.  Other frameworks, including single-shot detectors, also adopt anchor boxes to initialize regions of interest.&lt;/p&gt;

&lt;center&gt;
&lt;img src=&quot;https://kimfetti.github.io/images/anchors.png&quot; alt=&quot;Anchor boxes&quot; width=&quot;500&quot; /&gt;
&lt;p&gt;&lt;em&gt; Carefully chosen anchor boxes of varying sizes and aspect ratios help create diverse regions of interest.&lt;/em&gt;&lt;/p&gt;
&lt;/center&gt;

&lt;h4 id=&quot;multiple-feature-maps&quot;&gt;Multiple feature maps&lt;/h4&gt;

&lt;p&gt;Single-shot detectors must place special emphasis on the issue of multiple scales because they detect objects with a single pass through the CNN framework.  If objects are detected from the final CNN layers alone, only the largest objects will be found as smaller objects may lose signal during downsampling in the pooling layers.  To address this problem, single-shot detectors typically look for objects with multiple CNN layers including earlier layers where higher resolution remains.  Despite the precaution of using multiple feature maps, single-shot detectors are notoriously bad at detecting small objects, especially those in tight groupings like a flock of birds, though &lt;a href=&quot;https://arxiv.org/pdf/1804.02767.pdf&quot;&gt;recent YOLO developments&lt;/a&gt; appear promising.&lt;/p&gt;

&lt;center&gt;
&lt;img src=&quot;https://kimfetti.github.io/images/ssd.png&quot; alt=&quot;SSD with multiple feature maps&quot; width=&quot;800&quot; /&gt;
&lt;p&gt;&lt;em&gt; Feature maps from multiple CNN layers help predict objects at multiple scales.&lt;/em&gt;&lt;/p&gt;
&lt;/center&gt;

&lt;h4 id=&quot;feature-pyramid-network&quot;&gt;Feature pyramid network&lt;/h4&gt;

&lt;p&gt;The &lt;a href=&quot;https://arxiv.org/pdf/1612.03144.pdf&quot;&gt;feature pyramid network (FPN)&lt;/a&gt; takes the concept of multiple feature maps one step further.  Images first pass through the typical CNN pathway, yielding semantically rich final layers.  Then to regain better resolution, FPN creates a top-down pathway by upsampling the feature map.  While the top-down pathway helps detect objects of varying sizes, spatial positions may be skewed.  Lateral connections are added between the original feature maps and the corresponding reconstructed layers to improve object localization.  FPN currently provides one of the leading ways to detect objects at multiple scales, and YOLO was augmented with this technique in &lt;a href=&quot;https://arxiv.org/pdf/1804.02767.pdf&quot;&gt;version 3&lt;/a&gt;.&lt;/p&gt;

&lt;center&gt;
&lt;img src=&quot;https://kimfetti.github.io/images/fpn.png&quot; alt=&quot;Feature pyramid network&quot; width=&quot;450&quot; /&gt;
&lt;p&gt;&lt;em&gt; The feature pyramid network detects objects of varying sizes by reconstructing high resolution layers from layers with greater semantic strength.&lt;/em&gt;&lt;/p&gt;
&lt;/center&gt;

&lt;!--
### 3. Spatial position IS relevant 
- Image classification does not do this
- Fully connected layers (slow)
- ?? (I forgot this solution... )
--&gt;

&lt;h3 id=&quot;4-limited-data&quot;&gt;4. Limited data&lt;/h3&gt;

&lt;p&gt;Another substantial hurdle for object detection is the limited amount of annotated data currently available.  Object detection datasets typically contain ground truth examples for about a dozen to a hundred types of objects, while image classification datasets can include upwards of 100,000 different classes.  Furthermore, crowd sourcing often produces image classification tags for free (for example, by parsing the text of user-provided photo captions).  Gathering ground truth labels along with accurate bounding boxes for object detection, however, remains incredibly tedious work.&lt;/p&gt;

&lt;p&gt;The COCO dataset, provided by Microsoft, currently leads as one of the best object detection datasets.  COCO contains 300,000 segmented images with &lt;a href=&quot;https://github.com/pjreddie/darknet/blob/master/data/coco.names&quot;&gt;80 different categories&lt;/a&gt; of objects with very precise location labels.  Each image contains about 7 objects on average, and objects appear at very broad scales.  As helpful as this dataset is, object types outside of these 80 select items will not be recognized if training solely on COCO.&lt;/p&gt;

&lt;p&gt;A very interesting approach to ameliorate this issue comes from YOLO9000, the &lt;a href=&quot;https://arxiv.org/pdf/1612.08242.pdf&quot;&gt;second version of YOLO&lt;/a&gt;.  YOLO9000 incorporates many important updates into YOLO, but it also aims to narrow the dataset gap between object detection and image classification.  YOLO9000 trains simultaneously on both COCO and &lt;a href=&quot;http://www.image-net.org/&quot;&gt;ImageNet&lt;/a&gt;, an image classification dataset with tens of thousands of object classes.  COCO information helps precisely locate objects, while ImageNet increases YOLO’s classification “vocabulary.”  A hierarchical WordTree allows YOLO9000 to first detect an object’s concept (such as “animal/dog”) and to then drill down into specifics (such as “Siberian husky”).  This approach appears to work well for concepts known to COCO like animals but performs more poorly on concepts less prevalent in COCO like equipment or clothing.&lt;/p&gt;

&lt;center&gt;
&lt;img src=&quot;https://kimfetti.github.io/images/yolo9000.png&quot; alt=&quot;YOLO9000 WordTree and examples&quot; width=&quot;700&quot; /&gt;
&lt;p&gt;&lt;em&gt; YOLO9000 trains on both COCO and ImageNet to increase classification &quot;vocabulary.&quot;&lt;/em&gt;&lt;/p&gt;
&lt;/center&gt;

&lt;!--
- Lots of data for image classification (ImageNet), not so much for image detection (COCO)
- YOLO9000 attempt to leverage both for training
--&gt;

&lt;h3 id=&quot;5-class-imbalance&quot;&gt;5. Class imbalance&lt;/h3&gt;

&lt;p&gt;Class imbalance proves to be an issue for most classification problems, and object detection is no exception.  Consider a typical photograph.  More likely than not, the photograph contains a few main objects while the remainder of the image is filled with background.  Recall that selective search in R-CNN produces 2,000 candidate ROIs per image–a very large majority of these regions do not contain objects and are considered negatives.&lt;/p&gt;

&lt;!--
Rather than continuing to learn more about the background regions, hard example mining filters the negative examples done to those that the model performs worst one.  Some approaches also cap the ratio of picked negatives (background) to positives (objects), say, no greater than 3:1.

Non-maximal suppression (NMS) is also used by many object detection algorithms to correct for the fact that one object may be detected multiple times by the proposed RoIs.  With NMS the ....
--&gt;

&lt;p&gt;A recent approach called focal loss is implemented in &lt;a href=&quot;https://arxiv.org/abs/1708.02002&quot;&gt;RetinaNet&lt;/a&gt; and helps reduce the effects of class imbalance.  In the optimization loss function, focal loss replaces the traditional log loss used to penalize misclassifications:
\[ FL(p_u) = -\overbrace{(1-p_u)^\gamma\;}^{*} \log(p_u)\]
where \(p_u \) is the predicted class probability for the true class and \(\gamma &amp;gt; 0\).  The additional factor (*) reduces loss for well-classified examples with high probabilities. The overall effect thus deemphasizes classes with many examples that the model knows well, such as the background.  Though they occupy the minority classes, objects of interest receive more significance and see improved accuracy.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Object detection is customarily considered to be much harder than image classification, particularly because of these five challenges: dual priorities, speed, multiple scales, limited data, and class imbalance.  Researchers have dedicated much effort to face these challenges, yielding oftentimes amazing results.  Substantial challenges still persist, however.&lt;/p&gt;

&lt;p&gt;Basically all object detection frameworks continue to struggle with small objects, especially those bunched together with partial occlusions.  Real-time detection with top-level classification and localization accuracy remains challenging and practioners must often prioritize one or the other when making design decisions.  An interesting enhancement that may see more research in the future would extend the current two-dimensional bounding boxes into three-dimensional bounding cubes.  Furthermore, video tracking could see improvements if some continuity between frames were assumed rather than processing each frame individually.  Even though many object detection obstacles have seen creative solutions, these additional challenges–and plenty more–mean object detection research is certainly not done!&lt;/p&gt;

&lt;!--
- Much harder than image classification tasks
- Future challenges like adding LSTM, time component to video processing.  Currently one frame at a time
- Marry speed and accuracy AND extend object class space
- Also: pose, occlusions, lighting (same issues that image classification has but maybe even more so since also trying to localize object)
- Check out review's future stuff again
--&gt;

</content>

			
				<category term="algorithms" />
			
				<category term="literature reviews" />
			
			

			<published>2019-08-13T00:00:00-04:00</published>
		</entry>
	
		<entry>
			<id>http://localhost:4000//visualizations/matplotlib-improvements/</id>
			<title>Simple Ways to Improve Your Matplotlib</title>
			<link href="http://localhost:4000//visualizations/matplotlib-improvements/" rel="alternate" type="text/html" title="Simple Ways to Improve Your Matplotlib" />
			<updated>2019-08-12T00:00:00-04:00</updated>

			
				
				<author>
					
						<name>KFessel</name>
					
					
					
				</author>
			
			<summary>&lt;em&gt;Matplotlib's default properties often yield unappealing plots that can be off-putting to many users.  This post offers several simple ways to improve upon these defaults and help spruce up basic Matplotlib visualizations.&lt;/em&gt;</summary>
			<content type="html" xml:base="http://localhost:4000//visualizations/matplotlib-improvements/">&lt;!--more--&gt;

&lt;p&gt;&lt;a href=&quot;https://matplotlib.org/&quot;&gt;Matplotlib&lt;/a&gt; is typically the first data visualization package that Python programmers learn.  While its users can create basic figures with just a few lines of code, these resulting default plots often prove insufficient in both design aesthetics and communicative power.  Simple adjustments can lead to dramatic improvements, however, and in this post, I will share several tips on how to upgrade your Matplotlib figures.&lt;/p&gt;

&lt;p&gt;In the examples that follow, I will be using information found in &lt;a href=&quot;https://www.kaggle.com/crawford/80-cereals&quot;&gt;this Kaggle dataset about cereals&lt;/a&gt;.  I have normalized three features (calories, fat, and sugar) by serving size to better compare cereal nutrition and ratings.  Details about these data transformations and the code used to generate each example figure can be found on &lt;a href=&quot;https://github.com/kimfetti/Blog/blob/master/matplotlib_improvements.ipynb&quot;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;remove-spines&quot;&gt;Remove Spines&lt;/h2&gt;

&lt;p&gt;The first Matplotlib default to update is that black box surrounding each plot, comprised of four so-called “spines.”  To adjust them we first &lt;a href=&quot;https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.gca.html&quot;&gt;get our figure’s axes&lt;/a&gt; via pyplot and then change the visibility of each individual spine as desired.&lt;/p&gt;

&lt;p&gt;Let’s say, for example, we want to remove the top and right spines.  If we have imported Matplotlib’s pyplot submodule with:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pyplot&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;we just need to add the following to our code:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gca&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;spines&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'top'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_visible&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gca&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;spines&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'right'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_visible&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;and the top and right spines will no longer appear.  Removing these distracting lines allows more focus to be directed toward your data.&lt;/p&gt;

&lt;center&gt;
&lt;img src=&quot;https://kimfetti.github.io/images/spines.png&quot; alt=&quot;Remove matplotlib spines&quot; width=&quot;800&quot; /&gt;
&lt;p&gt;&lt;em&gt;Removing distracting spines can help people focus on your data.&lt;/em&gt;&lt;/p&gt;
&lt;/center&gt;

&lt;h2 id=&quot;explore-color-options&quot;&gt;Explore Color Options&lt;/h2&gt;

&lt;p&gt;Matplotlib’s &lt;a href=&quot;https://matplotlib.org/3.1.1/users/dflt_style_changes.html#colors-color-cycles-and-color-maps&quot;&gt;default colors just got an upgrade&lt;/a&gt; but you can still easily change them to make your plots more attractive or even to reflect your company’s brand colors.&lt;/p&gt;

&lt;h3 id=&quot;hex-codes&quot;&gt;Hex Codes&lt;/h3&gt;

&lt;p&gt;One of my favorite methods for updating Matplotlib’s colors is directly passing &lt;a href=&quot;https://htmlcolorcodes.com/&quot;&gt;hex codes&lt;/a&gt; into the color argument because it allows me to be extremely specific about my color choices.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scatter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'#0000CC'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;a href=&quot;https://www.w3schools.com/colors/colors_picker.asp&quot;&gt;This handy tool&lt;/a&gt; can help you select an appropriate hex color by testing it against white and black text as well as comparing several lighter and darker shades.  Alternatively, you can take a more scientific approach when choosing your palette by checking out &lt;a href=&quot;http://vrl.cs.brown.edu/color&quot;&gt;Colorgorical&lt;/a&gt; by Connor Gramazio from the Brown Visualization Research Lab.  The Colorgorical tool allows you to build a color palette by balancing various preferences like human perceptual difference and aesthetic pleasure.&lt;/p&gt;

&lt;h3 id=&quot;xkcd-colors&quot;&gt;xkcd Colors&lt;/h3&gt;

&lt;p&gt;The &lt;a href=&quot;https://xkcd.com/color/rgb/&quot;&gt;xkcd color library&lt;/a&gt; provides another great way to update Matplotlib’s default colors.  These 954 colors were specifically curated and named by several hundred thousand participants of the &lt;a href=&quot;https://blog.xkcd.com/2010/05/03/color-survey-results/&quot;&gt;xkcd color name survey&lt;/a&gt;.  You can use them in Matplotlib by prefixing their names with “xkcd:”.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scatter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'xkcd:lightish blue'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;center&gt;
&lt;img src=&quot;https://kimfetti.github.io/images/color.png&quot; alt=&quot;Explore matplotlib colors&quot; width=&quot;900&quot; /&gt;
&lt;p&gt;&lt;em&gt;Matplotlib's default colors can easily be updated by passing hex codes or referencing the xkcd library.&lt;/em&gt;&lt;/p&gt;
&lt;/center&gt;

&lt;h2 id=&quot;layer-graph-objects&quot;&gt;Layer Graph Objects&lt;/h2&gt;

&lt;p&gt;Matplotlib allows users to layer multiple graphics on top of each other, which proves convenient when comparing results or setting baselines.  Two useful properties should be utilized while layering: 1) &lt;code class=&quot;highlighter-rouge&quot;&gt;alpha&lt;/code&gt; for controlling each component’s opacity and 2) &lt;code class=&quot;highlighter-rouge&quot;&gt;zorder&lt;/code&gt; for moving objects to the foreground or background.&lt;/p&gt;

&lt;h3 id=&quot;opacity&quot;&gt;Opacity&lt;/h3&gt;

&lt;p&gt;The alpha property in Matplotlib adjusts an object’s opacity.  This value ranges from zero to one with zero being fully transparent (invisible 👀) and one being entirely opaque.  Reducing alpha will make your plot objects see-through, allowing multiple layers to be seen at once as well as allowing overlapping points to be distinguished, say, in a scatter plot.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scatter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;center&gt;
&lt;img src=&quot;https://kimfetti.github.io/images/alpha.png&quot; alt=&quot;Adjust matplotlib opacity&quot; width=&quot;800&quot; /&gt;
&lt;p&gt;&lt;em&gt;Decreasing alpha reduces opacity and can help you visualize overlapping points.&lt;/em&gt;&lt;/p&gt;
&lt;/center&gt;

&lt;h3 id=&quot;order&quot;&gt;Order&lt;/h3&gt;

&lt;p&gt;Matplotlib’s zorder property determines how close objects are to the foreground.  Objects with smaller zorder values appear closer to the background, while those with larger values present closer to the front.  If I’m making a scatter plot with an accompanying line plot, for example, I can bring the line forward by increasing its zorder.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scatter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;zorder&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  &lt;span class=&quot;c&quot;&gt;#background&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;zorder&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;     &lt;span class=&quot;c&quot;&gt;#foreground&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;center&gt;
&lt;img src=&quot;https://kimfetti.github.io/images/zorder.png&quot; alt=&quot;Control layer order with zorder&quot; width=&quot;600&quot; /&gt;
&lt;p&gt;&lt;em&gt; Plot objects can be brought to the foreground or pushed to the background by adjusting zorder.&lt;/em&gt;&lt;/p&gt;
&lt;/center&gt;

&lt;h2 id=&quot;annotate-main-points-or-examples&quot;&gt;Annotate Main Points or Examples&lt;/h2&gt;

&lt;p&gt;Many visuals can benefit from the annotation of main points or specific, illustrative examples because these directly convey ideas and boost the validity of results.  To add text to a Matplotlib figure, just include annotation code specifying the desired text and its location.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;annotate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;TEXT&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_POSITION&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Y_POSITION&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;The cereal dataset used to produced this blog’s visuals contains nutritional information about several brand name cereals along with a feature labeled as “rating.”  One might firstly assume that “rating” is a score indicating cereals that consumers prefer.  In the zorder figure above, however, I built a quick linear regression model showing that the correlation between calories per cup and rating is practically non-existent.  It seems unlikely that calories would not factor into consumer preference, so we may already be skeptical about our initial assumption about “rating.”&lt;/p&gt;

&lt;p&gt;This misconception becomes even more obvious when examining the extremes: Cap’n Crunch is the lowest rated cereal while All-Bran with Extra Fiber rates the highest.  Annotating the figure with these representative examples immediately dispels false assumptions about “rating.”  This rating information more likely indicates a cereal’s nutritional value. (I have also annotated the cereal with the most calories per cup; Grape Nuts is likely not meant to be consumed in such large quantities! 😆)&lt;/p&gt;

&lt;center&gt;
&lt;img src=&quot;https://kimfetti.github.io/images/annotate.png&quot; alt=&quot;Annotate examples&quot; width=&quot;700&quot; /&gt;
&lt;p&gt;&lt;em&gt; Annotating your visuals with a few examples can improve communication and add legitimacy.&lt;/em&gt;&lt;/p&gt;
&lt;/center&gt;

&lt;h2 id=&quot;baseline-and-highlight&quot;&gt;Baseline and Highlight&lt;/h2&gt;

&lt;p&gt;Adding a baseline to your visuals helps set expectations.  A simple horizontal or vertical line provides others with appropriate context and often speeds along their understanding of your results.  Highlighting a specific region of interest, meanwhile, can further emphasize your conclusions and also facilitates communication with your audience.  Matplotlib offers several options for baselining and highlighting, including horizontal and vertical lines, shapes such as rectangles, horizontal and vertical span shading, and filling between two lines.&lt;/p&gt;

&lt;h3 id=&quot;horizontal-and-vertical-lines&quot;&gt;Horizontal and Vertical Lines&lt;/h3&gt;

&lt;p&gt;Let’s now consider the interplay between fat and sugar in our cereal dataset.  A basic scatter plot of this relationship doesn’t appear interesting at first, but after exploring further, we find the median fat per cup of cereal is just one gram because so many cereals contain no fat at all.  Adding this baseline helps people arrive at this finding much more quickly.&lt;/p&gt;

&lt;center&gt;
&lt;img src=&quot;https://kimfetti.github.io/images/baseline.png&quot; alt=&quot;Add a baseline&quot; width=&quot;800&quot; /&gt;
&lt;p&gt;&lt;em&gt; A horizontal or vertical baseline can help set the stage for your data.&lt;/em&gt;&lt;/p&gt;
&lt;/center&gt;

&lt;p&gt;In other cases you may want to completely remove the default x- and y-axes that Matplotlib provides and create your own axes based on some data aggregate.  This process requires three key steps: 1) remove all default spines, 2) remove tick marks, and 3) add new axes as horizontal and vertical lines.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;#1. Remove spines&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;spine&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gca&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;spines&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;spine&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_visible&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;#2. Remove ticks&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xticks&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;yticks&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([])&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;#3. Add horizontal and vertical lines&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;axhline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y_POSITION&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  &lt;span class=&quot;c&quot;&gt;#horizontal line&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;axvline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_POSITION&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  &lt;span class=&quot;c&quot;&gt;#vertical line&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;center&gt;
&lt;img src=&quot;https://kimfetti.github.io/images/new_axes.png&quot; alt=&quot;Create new axes&quot; width=&quot;550&quot; /&gt;
&lt;p&gt;&lt;em&gt; You can also create new axes for your data by removing spines and ticks and adding custom lines.&lt;/em&gt;&lt;/p&gt;
&lt;/center&gt;

&lt;h3 id=&quot;rectangle&quot;&gt;Rectangle&lt;/h3&gt;

&lt;p&gt;Now that we have plotted the cereals’ fat and sugar contents on new axes, it appears that very few cereals are low in sugar but high in fat.  That is, the upper-left quadrant is nearly empty.  This seems reasonable because cereals typically are not savory.  To make this point abundantly clear, we could direct attention to this low-sugar, high-fat area by drawing a rectangle around it and annotating.  Matplotlib provides access to several shapes through its &lt;a href=&quot;https://matplotlib.org/3.1.1/api/patches_api.html#module-matplotlib.patches&quot;&gt;patches module&lt;/a&gt;, including a rectangle or even a &lt;a href=&quot;https://matplotlib.org/3.1.1/gallery/shapes_and_collections/dolphin.html#sphx-glr-gallery-shapes-and-collections-dolphin-py&quot;&gt;dolphin&lt;/a&gt;.  Begin by importing code for the rectangle:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.patches&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Rectangle&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Then to create a rectangle on the figure, grab the current axes and add a rectangular patch with its location, width, and height:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gca&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add_patch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Rectangle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_POSITION&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Y_POSITION&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;WIDTH&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;HEIGHT&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Here, the x- and y-positions refer to the placement of the lower-left corner of the rectangle.&lt;/p&gt;

&lt;center&gt;
&lt;img src=&quot;https://kimfetti.github.io/images/rectangle.png&quot; alt=&quot;Add a rectangle&quot; width=&quot;600&quot; /&gt;
&lt;p&gt;&lt;em&gt; To direct people toward a particular part of your visual, consider adding a rectangle.&lt;/em&gt;&lt;/p&gt;
&lt;/center&gt;

&lt;h3 id=&quot;shading&quot;&gt;Shading&lt;/h3&gt;

&lt;p&gt;Shading provides an alternative option for drawing attention to a particular region of your figure, and there are a few ways to add shading with Matplotlib.&lt;/p&gt;

&lt;p&gt;If you intend to highlight an entire horizontal or vertical area, just layer a span into your visual:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;axhspan&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y_START&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Y_END&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  &lt;span class=&quot;c&quot;&gt;#horizontal shading&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;axvspan&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_START&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_END&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  &lt;span class=&quot;c&quot;&gt;#vertical shading&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Previously discussed properties like &lt;code class=&quot;highlighter-rouge&quot;&gt;alpha&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;zorder&lt;/code&gt; are critical here because you will likely want to make your shading transparent and/or move it to the background.&lt;/p&gt;

&lt;center&gt;
&lt;img src=&quot;https://kimfetti.github.io/images/shading.png&quot; alt=&quot;Shading for highlighting&quot; width=&quot;800&quot; /&gt;
&lt;p&gt;&lt;em&gt; Shading also provides an effective way to highlight a particular region of your plot.&lt;/em&gt;&lt;/p&gt;
&lt;/center&gt;

&lt;p&gt;If the area you would like to shade follows more complicated logic, however, you may instead &lt;a href=&quot;https://matplotlib.org/api/_as_gen/matplotlib.pyplot.fill_between.html&quot;&gt;shade between two user-defined lines&lt;/a&gt;.  This approach takes a set of x-values, two sets of y-values for the first and second lines, and an optional &lt;code class=&quot;highlighter-rouge&quot;&gt;where&lt;/code&gt; argument that allows you to use logic to filter down to your region of interest.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gca&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fill_between&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_VALUES&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Y_LINE1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Y_LINE2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;WHERE&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;FILTER&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LOGIC&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;To shade the same area that was previously highlighted with a rectangle, simply define an array of equally spaced sugar values for the x-axis, fill between the median and max fat values on the y-axis (high fat), and filter down to sugar values less than the median (low sugar).&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;sugars&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linspace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sugars_per_cup&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;min&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sugars_per_cup&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gca&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fill_between&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sugars&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fat_per_cup&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;median&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fat_per_cup&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; 
                       &lt;span class=&quot;n&quot;&gt;WHERE&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sugars&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sugars_per_cup&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;median&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;center&gt;
&lt;img src=&quot;https://kimfetti.github.io/images/fill_between.png&quot; alt=&quot;Fill between lines&quot; width=&quot;600&quot; /&gt;
&lt;p&gt;&lt;em&gt; More complex shading logic is accomplished by filling between two lines and applying a filter.&lt;/em&gt;&lt;/p&gt;
&lt;/center&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Matplotlib often gets a bad reputation due to its poor defaults and the shear amount of code needed to produce decent looking visuals.  Hopefully, the tips provided in this blog will help you address the first issue, though I’ll admit that the final few example figures required many updates and subsequently a sizable amount of code.  If the required bulk of code bothers you, the &lt;a href=&quot;https://seaborn.pydata.org/&quot;&gt;Seaborn&lt;/a&gt; visualization library is an excellent alternative to Matplotlib.  It comes with better defaults overall, demands fewer lines of code, and supports customization via traditional Matplotlib syntax if needed.&lt;/p&gt;

&lt;p&gt;The main thing to keep in mind when you visualize data–no matter which package you choose–is your audience.  The suggestions I’ve offered here aim to smooth out the data communication process by 1) removing extraneous bits like unnecessary spines or tick marks, 2) telling the data story quicker by setting expectations with layering and baselines, and 3) highlighting main conclusions with shading and annotations.  The resulting aesthetics also improve, but the primary goal is stronger and more seamless data communication.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;I recently shared content similar to this in a data visualization talk at &lt;a href=&quot;https://odsc.com/training/portfolio/custom-data-visualizations-with-python/&quot;&gt;ODSC NYC&lt;/a&gt;.  You can access my original conference materials as well as the code that powers each example figure in the links below.&lt;/em&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/kimfetti/Blog/blob/master/matplotlib_improvements.ipynb&quot;&gt;Check out this code on GitHub!&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;http://bit.ly/odscNyc19_dataviz&quot;&gt;Check out my ODSC conference materials with Google Colab!&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

</content>

			
				<category term="visualizations" />
			
			

			<published>2019-08-12T00:00:00-04:00</published>
		</entry>
	
		<entry>
			<id>http://localhost:4000//mathematics/visualizations/planetary-birthday-problem/</id>
			<title>Solving the Birthday Problem on Mars</title>
			<link href="http://localhost:4000//mathematics/visualizations/planetary-birthday-problem/" rel="alternate" type="text/html" title="Solving the Birthday Problem on Mars" />
			<updated>2018-12-10T00:00:00-05:00</updated>

			
				
				<author>
					
						<name>KFessel</name>
					
					
					
				</author>
			
			<summary>&lt;em&gt;The birthday problem is a classic probability question with a surprising result.  In this post, we will solve this puzzler and extend the result by considering the answer for every planet in our solar system.&lt;/em&gt;</summary>
			<content type="html" xml:base="http://localhost:4000//mathematics/visualizations/planetary-birthday-problem/">&lt;!--more--&gt;

&lt;p&gt;I was recently asked to develop a challenge problem for the &lt;a href=&quot;https://www.thisismetis.com&quot;&gt;Metis&lt;/a&gt; data science bootcamp.  Perhaps it was my background in math or maybe my penchant for mild torture, but I decided to have students answer a few exercises from &lt;a href=&quot;https://www.amazon.com/Challenging-Problems-Probability-Solutions-Mathematics-ebook/dp/B00A3M0VV8&quot;&gt;Fifty Challenging Problems with Solutions&lt;/a&gt; by Mosteller.  This book is full of classic problems in probability, and I highly recommend it to anyone prepping for data science interviews!&lt;/p&gt;

&lt;p&gt;One of my favorite sections in this book is the birthday series, which includes a version of the birthday problem.  This problem is about as famous as a probability question can get.  It has been featured on &lt;a href=&quot;https://www.npr.org/templates/story/story.php?storyId=4542341&quot;&gt;NPR&lt;/a&gt;, written about in an &lt;a href=&quot;https://en.wikipedia.org/wiki/A_Fall_of_Moondust&quot;&gt;Arthur C. Clarke novel&lt;/a&gt;, and it even has its &lt;a href=&quot;https://en.wikipedia.org/wiki/Birthday_problem&quot;&gt;own Wikipedia page&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The problem goes something along the lines of:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;span class=&quot;teaser&quot;&gt;You are throwing a party and inviting random people you have never met. What’s the fewest number of &lt;br /&gt; people you need to invite to have at least 50% probability that two strangers will have the same birthday? &lt;br /&gt;(Birth year need not match.)&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;If you haven’t solved this one before, feel free to take a moment and give it a shot.  Be warned – &lt;strong&gt;spoilers ahead&lt;/strong&gt;!&lt;/p&gt;

&lt;h2 id=&quot;solving-with-probability&quot;&gt;Solving with Probability&lt;/h2&gt;

&lt;p&gt;Rather than the brute force approach, it turns out that the answer can be found much more easily by considering the complementary case; that is, “How many people can you invite to expect a 50% chance that all invited people have &lt;em&gt;unique&lt;/em&gt; birthdays?”  This “unsuccessful” probability along with the “successful” probability will sum to one.&lt;/p&gt;

&lt;p&gt;Keeping the complementary case in mind, note that the first person at your party can have their birthday on any calendar day, but after that, each person must have a different day.  Let \(p_u\) be the probability that \(r\) people each have unique birthdays. We find
\[p_u = 1 \cdot \frac{N-1}{N} \cdot \frac{N-2}{N} \cdots \frac{N-r+1}{N} = \frac{N!}{(N-r)!N^r}\]
where \(N\) is the number of days in one year.&lt;/p&gt;

&lt;p&gt;Backtracking to the original birthday problem, we now just need to find the minimum value of \(r\) people that satisfy:
\[p_{s} = 1 - \frac{N!}{(N-r)!N^r} &amp;gt; \frac{1}{2}.\]&lt;/p&gt;

&lt;p&gt;This expression doesn’t look all that pleasant to be solved outright, so instead we can build a little solver in Python--or the language of your choice!--to be able to compute \(p_s\) for any given \(r\) and \(N\).&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def prob_birthday_success(r, N=365):
    if r &amp;gt; N: 
        return 1.
    factorial = reduce(lambda x, y: x*y, range(N-r+1, N+1))
    power = N**r
    return (1 - factorial/power)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;We steadily increase \(r\) and once we hit the \(p_s \geq \frac{1}{2}\) mark, we have our desired party size.  The table below illustrates solutions for 50% probability as well as a few others values of \(p_s\).&lt;/p&gt;

&lt;center&gt;
&lt;table width=&quot;400&quot;&gt;
  &lt;caption&gt;Party size required for several success probabilities&lt;/caption&gt;
  &lt;colgroup&gt;
    &lt;col span=&quot;1&quot; style=&quot;width: 50%;&quot; /&gt;
    &lt;col span=&quot;1&quot; style=&quot;width: 50%;&quot; /&gt;
  &lt;/colgroup&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Probability&lt;/th&gt;
      &lt;th&gt;People Required&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;0.05&lt;/td&gt;
      &lt;td&gt;7&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;0.1&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;0.25&lt;/td&gt;
      &lt;td&gt;15&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;0.5&lt;/td&gt;
      &lt;td&gt;23&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;0.75&lt;/td&gt;
      &lt;td&gt;32&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;0.9&lt;/td&gt;
      &lt;td&gt;41&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;0.999&lt;/td&gt;
      &lt;td&gt;70&lt;/td&gt;
    &lt;/tr&gt;
   &lt;/tbody&gt;
&lt;/table&gt;
&lt;/center&gt;

&lt;p&gt;Notice that if we invite just 23 people to our party, we will have a 50-50 chance in finding a shared birthday.  Inviting 60 or 70 people pretty much guarantees it.  Incredible!&lt;/p&gt;

&lt;p&gt;So it’s plain to see that \(p_s\) increases rapidly as our party gets bigger here on Earth, but this led me to consider: “What would happen if the party took place on, say, Mars or Jupiter?”  Or in less whimsical terms: “How many people would we need if we varied the year length, \(N\)?”&lt;/p&gt;

&lt;h2 id=&quot;planetary-results&quot;&gt;Planetary Results&lt;/h2&gt;

&lt;p&gt;The first step in solving the birthday problem for the rest of our solar system is gathering &lt;a href=&quot;https://www.universetoday.com/37507/years-of-the-planets/&quot;&gt;year lengths for each planet&lt;/a&gt;, which vary wildly: from a meager 88 days on Mercury up to a whopping 60,182 days on Neptune.  In fact, your entire life will be confined to a single orbital period of Neptune.  There goes that Neptunian birthday party you’ve always wanted!  (Admittedly, the definition of a “birthday” gets a little murky on these other planets…  but more on this later.)&lt;/p&gt;

&lt;p&gt;Once year lengths have been gathered, working out the problem for different values of \(N\) is as simple as returning to the Python function introduced earlier.  The required number of party goers to achieve a 50% probability of birthday matching on each planet can be found below.  As a child of the 80s, I must tell you it is &lt;em&gt;VERY&lt;/em&gt; difficult for me to not include Pluto on this chart.  But there’s always hope for a &lt;a href=&quot;https://www.space.com/40550-pluto-planet-debate-flares-up-again.html&quot;&gt;Plutonian comeback&lt;/a&gt;! 🙏&lt;/p&gt;

&lt;center&gt;
&lt;iframe frameborder=&quot;no&quot; border=&quot;0&quot; marginwidth=&quot;0&quot; marginheight=&quot;0&quot; align=&quot;center&quot; width=&quot;1050&quot; height=&quot;500&quot; src=&quot;https://public.tableau.com/views/PlanetaryBirthdayProblem/Planets-50?:showVizHome=no&amp;amp;:embed=true&quot;&gt; &lt;/iframe&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Even on Neptune where there are over 60,000 Earth days per year, we only require 290 people to have a 50% chance of matching birthdays.  That’s an amazingly small amount for such a massive number of days in each year!&lt;/p&gt;

&lt;p&gt;So now back to our broader question: “What’s the overall trend as \(N\) increases?”  Well, the chart above is great for being able to read information associated with every planet, but it’s a bit misleading trendwise because both axes are on a log scale.  Let’s take a look at this same information without the axial scaling.&lt;/p&gt;

&lt;center&gt;
&lt;iframe frameborder=&quot;no&quot; border=&quot;0&quot; marginwidth=&quot;0&quot; marginheight=&quot;0&quot; align=&quot;center&quot; width=&quot;700&quot; height=&quot;400&quot; src=&quot;https://public.tableau.com/views/PlanetaryBirthdayProblem/Planets-50-Trueaxes?:showVizHome=no&amp;amp;:embed=true&quot;&gt; &lt;/iframe&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Ah ha!  Viewing the data this way, a trend that looks roughly like a power relationship emerges.  We will now take a closer look at the analytic expression for \(p_s\) to dive deeper into this relation.&lt;/p&gt;

&lt;h2 id=&quot;expanding-solution-with-approximations&quot;&gt;Expanding Solution with Approximations&lt;/h2&gt;

&lt;p&gt;We are about to embark upon the amazing world of expansions and approximations--AKA put your math pants on and fasten your seat belts!  (If this sort of nerdout isn’t your thing, no worries.  Just skip ahead to the end of this section where all will revealed… 🔮  Much of this work can also be found in &lt;a href=&quot;https://www.amazon.com/Challenging-Problems-Probability-Solutions-Mathematics-ebook/dp/B00A3M0VV8&quot;&gt;Mosteller&lt;/a&gt; as his solution to the birthday problem.)&lt;/p&gt;

&lt;p&gt;First recall that 
\[e^{-x} = 1 - x + \frac{x^2}{2!} - \frac{x^3}{3!}+ \cdots,\]
so ultimately, \(e^{-x} \approx 1 - x\) for very small values of \(x\).&lt;/p&gt;

&lt;p&gt;Now represent \(p_u\)--that’s the unsuccessful probability--as
\[p_u = \frac{N(N-1)(N-2)\cdots(N-r+1)}{N^r} = \frac{N^r - \hat{k}}{N^r} = 1 - \frac{k}{N}\]
where \(k\) contains multiple factors but all are smaller than \(N\).&lt;/p&gt;

&lt;p&gt;Combining these two expressions, we then find that
\[p_u = 1 - \frac{k}{N} \approx e^{-k/N},\]
which is valid because \(k/N\) is typically much smaller than one.&lt;/p&gt;

&lt;p&gt;Now let’s further consider the values contained within \(k\).  Expanding out the numerator in \(p_u\), we have
\[p_u=\frac{N(N-1)\cdots(N-r+1)}{N^r} = \frac{N^r - N^{r-1}\left[0+1+2+\cdots(r-1)\right] + \cdots}{N^r} = 1 - \frac{0 + 1 + 2 + \cdots (r-1)}{N} + \cdots.\]
The quantity \(k\) represents several terms, but to leading order, it just looks a sum of the integers between 0 and \(r-1\).  More specificially,
\[k \approx 0 + 1 + 2 + \cdots + r-1 = \sum_{i=0}^{r-1}j = \frac{r(r-1)}{2}.\]
So where does this lead us?  Returning to our exponential expression above, we have
\[p_u \approx e^{-k/N} \approx e^{-r(r-1)/2N},\]
which looks &lt;em&gt;much&lt;/em&gt; more tractable than the original expression we had for \(p_u\) containing those factorials.  We can even come up with an expression to relate \(r\) to \(N\) more explicitly in the leading order.&lt;/p&gt;

&lt;p&gt;Subbing in \(p_u = 1 - p_s\) and taking the natural log of each side, we eventually find
\[\frac{r(r-1)}{2N} \approx -\ln{(1 - p_s)},\]
which means
\[r(r-1) \approx -2N\ln{(1-p_s)}.\]
So there you have it!  Selecting in any given value for \(p_s\) will fix the log factor and the other two quantities are related as
\[\mathcal{O}\left(r\right) \sim \mathcal{O}\left(\sqrt{N}\right).\]
The trend we saw in the planet chart was indeed a power relationship; specifically, \(r\) goes like \(\sqrt{N}\) as \(N\) increases for the birthday problem.  This means that even on planets with many, many days in a year, we don’t really need to increase our party size by all that much to ensure our 50-50 chance of finding birthday twins.&lt;/p&gt;

&lt;h2 id=&quot;approximation-in-action&quot;&gt;Approximation in Action&lt;/h2&gt;
&lt;p&gt;How good is this approximation in practice?  Well, the trendline we saw earlier in the true-scale axes chart was auto-fitted in Tableau with a power trend, and indeed, the equation for the resulting line was found to be&lt;/p&gt;

&lt;p&gt;\[r = 1.28548 \cdot N^{0.491503}\]&lt;/p&gt;

&lt;p&gt;So our square-root relationship appears to hold true.&lt;/p&gt;

&lt;p&gt;We can also more explicitly consider what happens as \(N\) gets larger. Because we are estimating
\[1-\frac{k}{N} \approx e^{-k/N},\]
this approximation should actually become &lt;em&gt;more&lt;/em&gt; valid as \(N\) becomes larger since \(k/N\) will resultingly grow smaller.&lt;/p&gt;

&lt;p&gt;Now set \(p_s = \frac{1}{2}\) and let \(r_{1/2}\) be the 50-50 chance party size. Plotting the left-hand side of our approximation
\[\frac{r_{1/2}(r_{1/2}-1)}{2N} \approx \ln{2}\]
for various evenly sampled values of \(N\), we see in the figure below that this estimation indeed becomes more valid and encounters less variance about \(\ln{(2)}\) as \(N \to \infty\).  (There is an added layer of complexity in this problem, however, because we require \(r_{1/2}\) to be an integer; this stipulation makes our approximation dance about the line a bit even at large values of \(N\).)&lt;/p&gt;

&lt;center&gt;
&lt;img src=&quot;https://kimfetti.github.io/images/planetary-birthday-approx.png&quot; alt=&quot;Approximation plot&quot; width=&quot;900&quot; height=&quot;400&quot; /&gt;
&lt;/center&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;The birthday problem is a classic that has been examined from several different angles.  I hope you’ve enjoyed this planetary rendition and the subsequent deep dive into analytic approximations to explore how \(r\) is related to year length.&lt;/p&gt;

&lt;p&gt;A few final thoughts:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;It is well-known that birthdays are &lt;a href=&quot;http://www.panix.com/~murphy/bday.html&quot;&gt;not equally distributed throughout all 365 days&lt;/a&gt;, especially if you focus on one region of the world.  So how does non-uniformity affect our birthday solution?&lt;/p&gt;

    &lt;p&gt;It turns out that the uniform distribution of birthdays we used throughout this post is actually a &lt;a href=&quot;https://www.jstor.org/stable/2318556?seq=1#page_scan_tab_contents&quot;&gt;worst-case scenario&lt;/a&gt; in terms of successfully finding birthmates.  If birthdays are skewed toward one day or another, the odds that you will find birthday twins at your party actually increase… but not significantly.  Attempts at calculating the birthday problem with real-world datasets have shown the 23-person group to be a pretty consistent solution, even when considering &lt;a href=&quot;https://www.stat.wisc.edu/techreports/tr591.pdf&quot;&gt;non-uniform distributions&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;I eluded to this earlier, but the idea of a “birthday” gets a bit complicated when thinking about other planets.  I often mentioned my findings in terms of “Earth days” because I calculated each planet’s revolution about the Sun in the number of times it takes Earth to rotate about its own axis.  What does that mean in the context of this problem?&lt;/p&gt;

    &lt;p&gt;Consider two cases: Jupiter and Mercury.&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;Firstly, Jupiter rotates about its own axis in about &lt;a href=&quot;https://www.universetoday.com/37507/years-of-the-planets/&quot;&gt;9 hours and 55 minutes&lt;/a&gt;, faster than any other planet in our solar system.  So while Jupiter takes roughly 4,333 Earth days to complete its orbit about the Sun, this actually amounts to 10,476 &lt;em&gt;Jovian&lt;/em&gt; days.  That’s a lot more potential “birthdays!”&lt;/li&gt;
      &lt;li&gt;Mercury, on the other hand, completes a rotation about its axis &lt;em&gt;slower&lt;/em&gt; than any other planet. It takes about 176 Earth days for Mercury to rotate, which is &lt;em&gt;longer&lt;/em&gt; than Mercury’s revolution about the Sun.  Ultimately, a “year” on Mercury is half as long as a “day.”  The birthday problem is completely moot because everyone born in the same Mercurian year is automatically born on the same Mercurian day! (… Please ignore the fact that no one is ever actually born on Mercury. 😆)&lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;&lt;br /&gt;
 I have chosen to disregard this planetary difference in the definition of a “day” for simplicity, but keeping with the same Python function introduced near the beginning of this post, we could relatively easily compute revised birthday solutions.  Please let me know if you work this out!&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/kimfetti/Blog/blob/master/planetary_birthday_problem.ipynb&quot;&gt;Check out this code on GitHub!&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://public.tableau.com/profile/kimberly.fessel#!/vizhome/PlanetaryBirthdayProblem/Planets-50&quot;&gt;Check out this viz on Tableau!&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

</content>

			
				<category term="mathematics" />
			
				<category term="visualizations" />
			
			

			<published>2018-12-10T00:00:00-05:00</published>
		</entry>
	
</feed>